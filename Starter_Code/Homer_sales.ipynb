{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4364863f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import findspark and initialize\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Import necessary PySpark SQL functions\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQL_HomeSales\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20aafb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the home_sales_revised.csv into a Spark DataFrame\n",
    "file_path = '/mnt/data/home_sales_revised.csv'\n",
    "home_sales_df = spark.read.csv(file_path, header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb38782",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a temporary view of the DataFrame\n",
    "home_sales_df.createOrReplaceTempView(\"home_sales\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2d127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. What is the average price for a four-bedroom house sold per year, rounded to two decimal places?\n",
    "avg_price_4_bed = spark.sql('''\n",
    "    SELECT \n",
    "        year(date) as year_sold, \n",
    "        ROUND(AVG(price), 2) as avg_price \n",
    "    FROM \n",
    "        home_sales \n",
    "    WHERE \n",
    "        bedrooms = 4 \n",
    "    GROUP BY \n",
    "        year_sold \n",
    "    ORDER BY \n",
    "        year_sold\n",
    "''')\n",
    "avg_price_4_bed.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c3693",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. What is the average price of a home for each year the home was built, that has three bedrooms and three bathrooms?\n",
    "avg_price_3_bed_3_bath = spark.sql('''\n",
    "    SELECT \n",
    "        year(date_built) as year_built, \n",
    "        ROUND(AVG(price), 2) as avg_price \n",
    "    FROM \n",
    "        home_sales \n",
    "    WHERE \n",
    "        bedrooms = 3 \n",
    "        AND bathrooms = 3 \n",
    "    GROUP BY \n",
    "        year_built \n",
    "    ORDER BY \n",
    "        year_built\n",
    "''')\n",
    "avg_price_3_bed_3_bath.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c621936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. What is the average price of a home for each year the home was built, that has three bedrooms, three bathrooms, two floors, and is >= 2,000 sq ft?\n",
    "avg_price_specific = spark.sql('''\n",
    "    SELECT \n",
    "        year(date_built) as year_built, \n",
    "        ROUND(AVG(price), 2) as avg_price \n",
    "    FROM \n",
    "        home_sales \n",
    "    WHERE \n",
    "        bedrooms = 3 \n",
    "        AND bathrooms = 3 \n",
    "        AND floors = 2 \n",
    "        AND sqft_living >= 2000 \n",
    "    GROUP BY \n",
    "        year_built \n",
    "    ORDER BY \n",
    "        year_built\n",
    "''')\n",
    "avg_price_specific.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df085223",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. What is the average price of a home per \"view\" rating having an average home price >= $350,000?\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "avg_price_per_view = spark.sql('''\n",
    "    SELECT \n",
    "        view, \n",
    "        ROUND(AVG(price), 2) as avg_price \n",
    "    FROM \n",
    "        home_sales \n",
    "    GROUP BY \n",
    "        view \n",
    "    HAVING \n",
    "        avg_price >= 350000 \n",
    "    ORDER BY \n",
    "        avg_price DESC\n",
    "''')\n",
    "avg_price_per_view.show()\n",
    "\n",
    "print(\"Runtime: %s seconds\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59be79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. Cache the temporary table home_sales\n",
    "spark.sql(\"CACHE TABLE home_sales\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf84b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8. Verify the table is cached\n",
    "is_cached = spark.catalog.isCached(\"home_sales\")\n",
    "print(f\"Is 'home_sales' table cached? {is_cached}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cebe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 9. Run the last query on cached data and compare the runtime\n",
    "start_time_cached = time.time()\n",
    "\n",
    "avg_price_per_view_cached = spark.sql('''\n",
    "    SELECT \n",
    "        view, \n",
    "        ROUND(AVG(price), 2) as avg_price \n",
    "    FROM \n",
    "        home_sales \n",
    "    GROUP BY \n",
    "        view \n",
    "    HAVING \n",
    "        avg_price >= 350000 \n",
    "    ORDER BY \n",
    "        avg_price DESC\n",
    "''')\n",
    "avg_price_per_view_cached.show()\n",
    "\n",
    "print(\"Cached Runtime: %s seconds\" % (time.time() - start_time_cached))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5c8811",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 10. Partition by the \"date_built\" field on the formatted parquet home sales data\n",
    "home_sales_df.write.partitionBy(\"date_built\").parquet(\"home_sales_partitioned\")\n",
    "\n",
    "# Read the partitioned parquet data\n",
    "partitioned_df = spark.read.parquet(\"home_sales_partitioned\")\n",
    "partitioned_df.createOrReplaceTempView(\"home_sales_partitioned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdd8702",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 11. Run the last query on the partitioned parquet temporary table and compare the runtime\n",
    "start_time_partitioned = time.time()\n",
    "\n",
    "avg_price_per_view_partitioned = spark.sql('''\n",
    "    SELECT \n",
    "        view, \n",
    "        ROUND(AVG(price), 2) as avg_price \n",
    "    FROM \n",
    "        home_sales_partitioned \n",
    "    GROUP BY \n",
    "        view \n",
    "    HAVING \n",
    "        avg_price >= 350000 \n",
    "    ORDER BY \n",
    "        avg_price DESC\n",
    "''')\n",
    "avg_price_per_view_partitioned.show()\n",
    "\n",
    "print(\"Partitioned Runtime: %s seconds\" % (time.time() - start_time_partitioned))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d7e632",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 12. Uncache the home_sales temporary table\n",
    "spark.sql(\"UNCACHE TABLE home_sales\")\n",
    "\n",
    "# Verify that the table is uncached\n",
    "is_uncached = not spark.catalog.isCached(\"home_sales\")\n",
    "print(f\"Is 'home_sales' table uncached? {is_uncached}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
